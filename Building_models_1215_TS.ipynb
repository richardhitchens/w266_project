{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.stats as stats\n",
    "import datetime\n",
    "import pylab as pl\n",
    "import math\n",
    "import codecs\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.matutils import hellinger\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import logging\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all important file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Richard path specs\n",
    "\n",
    "# # At work:\n",
    "# TEXT_PATH = \"T:/Quant/TextAnalysis/Transcripts/SP100/Text/\"\n",
    "# PDF_PATH = \"T:/Quant/TextAnalysis/Transcripts/SP100/PDF/\"\n",
    "# LIBRARY_PATH = \"T:/Quant/TextAnalysis/Transcripts/SP100/Libraries/\"\n",
    "\n",
    "# # At home:\n",
    "# dict_path = '/Users/Richard/Desktop/Berkeley/w266/'\n",
    "# input_path = '/Users/Richard/Desktop/Berkeley/w266/repo/w266_project/'\n",
    "# output_path = '/Users/Richard/Desktop/Berkeley/w266/'\n",
    "\n",
    "# LIBRARY_PATH = '/Users/Richard/Desktop/Berkeley/w266/repo/w266_project/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tom path specs\n",
    "\n",
    "# Tom machine:\n",
    "TEXT_DIR_LIST = [\"T1\", \"T2\", \"T3\", \"T4\"]\n",
    "# PDF_PATH = \"T:/Quant/TextAnalysis/Transcripts/SP100/PDF/\"\n",
    "LIBRARY_PATH = \"/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/\"\n",
    "\n",
    "# On Tom's google cloud instance\n",
    "# LIBRARY_PATH = \"/home/seddon/w266_project/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a dictionary\n",
    "\n",
    "Import a bag of words style dictionary for word counting analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dictionary(file_path, file_name):\n",
    "    \"\"\"Read a standard word list dictionary text file into a list\"\"\"\n",
    "    with open(file_path + file_name, \"r\") as file:\n",
    "#         words = [word.lower().rstrip('\\n') for word in file]\n",
    "        words = [word.lower().rstrip() for word in file] # had to strip \\r as well on my machine\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a list of files that have valid Q&A sections we want to use\n",
    "\n",
    "This section reviews all the files and rejects any that do not meet our criteria for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Different possible section headers\n",
    "CO_PART_HEADERS = ['Company Participants']\n",
    "OTH_PART_HEADERS = ['Other Participants']\n",
    "MD_SECTION_HEADERS = ['MANAGEMENT DISCUSSION SECTION', 'Presentation']\n",
    "QA_SECTION_HEADERS = ['Q&A', 'Questions And Answers', 'QUESTION AND ANSWER SECTION',\n",
    "                      'QUESTION AND ANSWER SESSION', 'QUESTION-AND-ANSWER SECTION']\n",
    "DISCLAIMER = ['This transcript may not be 100 percent accurate ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# New text file format has headers\n",
    "def remove_header_footers(ts):\n",
    "    i = 0\n",
    "    header_footer = []\n",
    "    # Add all the lines before the first 'Page 1 of n' line\n",
    "    while ts[i].find('Page') == -1:\n",
    "#         if ts[i].find('Fixed') != -1:\n",
    "#             print (ts[1], ts[2])\n",
    "        header_footer.append(ts[i])\n",
    "        i += 1\n",
    "    # Extract the total page number and add lines to strip for all the \n",
    "    # 'Page x of n' line\n",
    "    pages = ts[i]\n",
    "    pages = pages.split(' ')\n",
    "    page_num = int(pages[-1])\n",
    "    for i in range(1, page_num + 1):\n",
    "        header_footer.append('Page ' + str(i) + ' of ' + str(page_num))\n",
    "    ts = [line for line in ts if line not in header_footer]\n",
    "    return ts\n",
    "\n",
    "\n",
    "def read_transcript(file_path):\n",
    "    # read in all lines of the transcript\n",
    "    with open(file_path, \"r\") as file:\n",
    "        ts = file.readlines()\n",
    "        ts = [str(unicode(line, errors = 'ignore')) for line in ts]\n",
    "        ts = [line.rstrip() for line in ts]\n",
    "        ts = remove_header_footers(ts)\n",
    "    return ts\n",
    "\n",
    "\n",
    "# Find the index value of a full string from a list of possible strings\n",
    "def find_full_string_index(ts, string_list):\n",
    "    result = -1\n",
    "    for i in range(len(string_list)):\n",
    "        if string_list[i] in ts:\n",
    "            result = ts.index(string_list[i])\n",
    "    return result\n",
    "\n",
    "# Find the index of the disclaimer\n",
    "def find_disclaimer_index(ts, string_list):\n",
    "    result = -1\n",
    "    k = len(ts) - 1\n",
    "    while True:\n",
    "        for my_str in string_list:\n",
    "            if ts[k].find(my_str) != -1:\n",
    "                result = k\n",
    "                break\n",
    "        k -= 1\n",
    "        if k < 0:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# Get transcript list indices from a single transcript\n",
    "def get_basic_parameters(ts):\n",
    "    call_type = ts[0]\n",
    "    # check call type\n",
    "    if call_type.find(\"Earnings Call\") == -1:\n",
    "        call_type = -1\n",
    "    else:\n",
    "        if call_type.find(\"Fixed\") != -1:  # don't want Fixed Income calls\n",
    "            call_type = -1\n",
    "            \n",
    "    co_parts = find_full_string_index(ts,CO_PART_HEADERS)\n",
    "    oth_parts = find_full_string_index(ts,OTH_PART_HEADERS)\n",
    "    md = find_full_string_index(ts,MD_SECTION_HEADERS)\n",
    "    qa = find_full_string_index(ts,QA_SECTION_HEADERS)\n",
    "    disc = find_disclaimer_index(ts,DISCLAIMER)\n",
    "    return [call_type, co_parts, oth_parts, md, qa, disc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Files found:  3245\n",
      "\n",
      "Files rejected: 103\n",
      "\n",
      "Number of files with each type of error (file can have >1 error):\n",
      "no Q&A section 42\n",
      "Files are:\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100806_1_AIG.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100817_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101105_1_AIG.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101110_1_GM.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101116_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110517_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110816_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20111115_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120221_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120517_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120816_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20121115_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130221_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130730_1_NYX.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130815_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130911_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130923_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20131114_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20131220_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140220_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140320_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140515_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140522_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140725_1_ABBV.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140814_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140820_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20141113_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20141119_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150219_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150313_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150515_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150519_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150814_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150814_1_KHC.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150818_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20151117_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160218_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160519_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160818_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20161117_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20110222_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20170221_1_WMT.txt\n",
      "not equity earnings call 59\n",
      "Files are:\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101019_1_AEP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101022_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101026_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110121_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110426_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110427_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110721_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110728_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20111020_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120124_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120425_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120427_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120719_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120720_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120724_1_GS.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120725_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20121024_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20121030_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130123_1_GS.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130125_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130129_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130422_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130424_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130718_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130719_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130724_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130909_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20131023_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20131024_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20131106_1_GS.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140123_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140128_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140417_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140501_1_GS.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140718_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140724_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20141017_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20141024_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20150123_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150129_1_GS.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150423_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150428_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150721_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150728_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150728_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150804_1_GS.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20151027_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20151029_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160121_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160128_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160421_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160721_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160721_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160802_1_GS.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20161026_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20170126_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20110128_1_F.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20170420_1_C.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20170720_1_C.txt\n",
      "no other participants 48\n",
      "Files are:\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100806_1_AIG.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100817_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101019_1_AEP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101105_1_AIG.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101110_1_GM.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20101116_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110517_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110728_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20110816_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20111115_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120221_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120517_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20120816_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20121115_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130221_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130718_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130730_1_NYX.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130815_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130911_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20130923_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20131114_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20131220_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140220_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140320_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140515_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140522_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140725_1_ABBV.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140814_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20140820_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20141113_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T2/20141119_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150219_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150313_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150515_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150519_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150814_1_HNZ.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150814_1_KHC.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20150818_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20151117_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160218_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160519_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160721_1_AXP.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20160818_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T3/20161117_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20110222_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20170221_1_WMT.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20170620_1_FDX.txt\n",
      "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T4/20170919_1_FDX.txt\n"
     ]
    }
   ],
   "source": [
    "# Get the file paths to all files of a specified file type from a given directory\n",
    "def get_files(file_path, file_type):\n",
    "    file_list = glob.glob(file_path + \"*.\" + file_type)\n",
    "    return file_list\n",
    "\n",
    "def get_files_from_dirs(directory_list, file_type):\n",
    "    # Read in all the files from subdirectories into a master list\n",
    "    \n",
    "    matchfiles = []\n",
    "    for d in directory_list:\n",
    "        matchfiles.extend(get_files(LIBRARY_PATH+d+\"/\",'txt'))    \n",
    "    return matchfiles\n",
    "\n",
    "def find_valid_files(file_list):\n",
    "    '''Scans all the files in the file list and returns a list of those which\n",
    "       pass all QA checks, and a list of those that do not, with reason why'''\n",
    "    \n",
    "    good_files = []\n",
    "    problems = []\n",
    "\n",
    "    for path in file_list:\n",
    "        ts = read_transcript(path)\n",
    "        params = get_basic_parameters(ts)\n",
    "        \n",
    "        errs = [\"not equity earnings call\",\n",
    "                \"no co participants\",\n",
    "                \"no other participants\",\n",
    "                \"no mgmt discussion section\",\n",
    "                \"no Q&A section\",\n",
    "                \"no disclaimer\"\n",
    "                ]\n",
    "        \n",
    "        errs_found = []\n",
    "        for i, err in enumerate(errs):\n",
    "            if params[i] == -1:\n",
    "                errs_found.append(err)\n",
    "        \n",
    "        if len(errs_found) == 0:\n",
    "            good_files.append(path)\n",
    "        else:\n",
    "            problems.append((errs_found, path, params))\n",
    "\n",
    "    return good_files, problems\n",
    "\n",
    "\n",
    "files_to_check = get_files_from_dirs(TEXT_DIR_LIST, \"txt\")\n",
    "TextFiles, error_files = find_valid_files(files_to_check)\n",
    "\n",
    "print \"Valid Files found: \", len(TextFiles)\n",
    "print\n",
    "print \"Files rejected:\", len(error_files)\n",
    "print\n",
    "err_counts = defaultdict(int)\n",
    "err_lists = defaultdict(list)\n",
    "for f in error_files:\n",
    "    for err in f[0]:\n",
    "        err_counts[err] += 1\n",
    "        err_lists[err].append(f[1])\n",
    "\n",
    "print \"Number of files with each type of error (file can have >1 error):\"\n",
    "for err in err_counts:\n",
    "    print err, err_counts[err]\n",
    "    print \"Files are:\"\n",
    "    for f in err_lists[err]:\n",
    "        print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100730_1_AEP.txt',\n",
       " '/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100730_1_CVX.txt',\n",
       " '/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100730_1_MET.txt',\n",
       " '/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100730_1_MRK.txt',\n",
       " '/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/T1/20100730_1_SPG.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextFiles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_square_brackets(my_str):\n",
    "    return re.sub(r'\\[.+?\\]', '', my_str)\n",
    "\n",
    "def remove_square_brackets_transcript(ts):\n",
    "    return list(map(lambda line: remove_square_brackets(line), ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find participants on the call\n",
    "def get_parts(ts, start_index, end_index):\n",
    "    result = [name[re.search(\"[a-z]\", name.lower()).start():] for name in ts[start_index+1:end_index] if len(name) >= 3]\n",
    "    # Check for problematic names\n",
    "    extra_names = []\n",
    "    for name in result:\n",
    "        # Find any title after name and remove it\n",
    "        if name.find(',') != -1:\n",
    "            name_only = name[:name.find(',')]\n",
    "            extra_names.append(name_only)\n",
    "        # Find names with middle initials and remove them\n",
    "        elif name.find('.') != -1:\n",
    "            name_only = remove_middle_initials(name)\n",
    "            if name_only != name:\n",
    "                extra_names.append(name_only)\n",
    "    # Find names in extra names with middle initials and remove them\n",
    "    for name in extra_names:\n",
    "        if name.find('.') != -1:\n",
    "            name_only = remove_middle_initials(name)\n",
    "            if name_only != name:\n",
    "                extra_names.append(name_only)\n",
    "    return result + extra_names\n",
    "\n",
    "# Remove middle initial from a name\n",
    "def remove_middle_initials(my_str):\n",
    "    # count = 0\n",
    "    while True:\n",
    "        if my_str.find('.') != -1:\n",
    "            new_my_str = re.sub(r'(\\s)([A-Z].)(\\s)', r\"\\1\", my_str)\n",
    "            if new_my_str == my_str:\n",
    "                break\n",
    "            else:\n",
    "                my_str = new_my_str\n",
    "            # count += 1\n",
    "            # if count >= 10:\n",
    "            #     break\n",
    "        else:\n",
    "            break\n",
    "    return my_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find a section of the call\n",
    "def get_section(ts, start_index, end_index):\n",
    "    result = ts[start_index+1:end_index]\n",
    "    result = [line for line in result if line not in ['\\x0c']]   # clean the transcript for unnecessary lines\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_MD(ts, names, start_index, end_index):\n",
    "\n",
    "    curr_speaker = \"\"\n",
    "    section_header = \"MD\"\n",
    "    \n",
    "    # Create an empty dict for comments by each manager\n",
    "    pts = {name: \"\" for name in names}\n",
    "\n",
    "    # Get the required section\n",
    "    section = get_section(ts, start_index, end_index)\n",
    "    \n",
    "    # Find the first instance of a co_parts speaker\n",
    "    start_section = 0\n",
    "    while True:\n",
    "        if section[start_section] not in names:\n",
    "            start_section += 1\n",
    "        else:\n",
    "            break\n",
    "        if start_section == len(section):\n",
    "            start_section = -1\n",
    "            break\n",
    "\n",
    "    if start_section == -1:\n",
    "        pts['No speaker found'] = section_header\n",
    "    else:\n",
    "        # Reduce MD Section to management speaking only\n",
    "        section = section[start_section:]\n",
    "        # populate comment dict\n",
    "        for line in section:\n",
    "            if line in names:\n",
    "                curr_speaker = line\n",
    "            elif len(line) != 0:\n",
    "                pts[curr_speaker] += line + \" \"\n",
    "\n",
    "    return pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This procedure combines the raw Q&A section into pairs of questions and answers in a dictionary\n",
    "\n",
    "def get_QA(ts, start_index, end_index):\n",
    "\n",
    "    pts = {}\n",
    "    # get the Q&A section\n",
    "    qa_section = get_section(ts, start_index, end_index)\n",
    "\n",
    "    q_or_a = None\n",
    "    currQ = ''\n",
    "    currA = ''\n",
    "    \n",
    "    for line in qa_section:\n",
    "        line = remove_square_brackets(line)\n",
    "        if len(line) > 0:                                     # if line is '' then ignore otherwise process the line\n",
    "            if line == 'Operator':                            # if line is 'Operator' then set everything NULL and ignore\n",
    "                q_or_a = None\n",
    "                currQ = ''\n",
    "                currA = ''\n",
    "            if line[0] == \"<\":                                # if Q or A found do something\n",
    "                if line[0:2] == \"<Q\":\n",
    "                    if q_or_a == 'A':                         # if Q found and q_or_a is not NULL then add Q: A to the dictionary\n",
    "                        pts[currQ[:-1]] = currA[:-1]          # and reset the parameters to NULL\n",
    "                        currQ = ''\n",
    "                        currA = ''\n",
    "                        q_or_a = None    \n",
    "                    end_qa_tag = line.find(\">\")\n",
    "                    currQ += line[end_qa_tag + 3:] + \" \"      # Concatenate the string after the '>' to the previous string\n",
    "                    q_or_a = 'Q'                              # Given Q found set q_or_a = 'Q'\n",
    "                if line[0:2] == \"<A\":\n",
    "                    end_qa_tag = line.find(\">\")             \n",
    "                    currA += line[end_qa_tag + 3:] + \" \"      # Concatenate the string after the '>' to the previous string\n",
    "                    q_or_a = 'A'                              # Given Q found set q_or_a = 'Q'\n",
    "            else:                                             \n",
    "                if q_or_a is not None:                        # Other if currently a Q or A just concatenate the string \n",
    "                    if q_or_a == 'Q':\n",
    "                        currQ += line + \" \"\n",
    "                    else:\n",
    "                        currA += line + \" \"\n",
    "\n",
    "    if currQ != '' and currA != '':                           # Add last Q: A pair to the dictionary if necessary\n",
    "        pts[currQ[:-1]] = currA[:-1]\n",
    "\n",
    "    return pts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bespoke cleaning\n",
    "\n",
    "The following procedures deal with strings with words that have apostrophes, hyphens, slashes and unusual characters in them to help best isolate actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def short_form_replace(my_str):\n",
    "    \"\"\"Convert apostrophes in known short-forms in my_string to long-forms\"\"\"\n",
    "    my_str = replace_non_utf8_apostrophes(my_str)\n",
    "    my_str = re.sub(r'let\\'s', \"let us\", my_str)\n",
    "    my_str = re.sub(r'Let\\'s', \"let us\", my_str)\n",
    "    my_str = re.sub(r'won\\'t', \"will not\", my_str)\n",
    "    my_str = re.sub(r'can\\'t', \"cannot\", my_str)\n",
    "    my_str = re.sub(r'shan\\'t', \"shall not\", my_str)\n",
    "    my_str = re.sub(r'Won\\'t', \"Will not\", my_str)\n",
    "    my_str = re.sub(r'Can\\'t', \"Cannot\", my_str)\n",
    "    my_str = re.sub(r'Shan\\'t', \"Shall not\", my_str)\n",
    "    my_str = re.sub(r'n\\'t', \" not\", my_str)\n",
    "    my_str = re.sub(r'\\'ve', \" have\", my_str)\n",
    "    my_str = re.sub(r'\\'re', \" are\", my_str)\n",
    "    my_str = re.sub(r'\\'m', \" am\", my_str)\n",
    "    my_str = re.sub(r'\\'ll', \" will\", my_str)\n",
    "    my_str = re.sub(r'\\'d', \" would\", my_str)     # note could also be did or had as well\n",
    "    my_str = re.sub(r'it\\'s', \"it is\", my_str)\n",
    "    my_str = re.sub(r'he\\'s', \"he is\", my_str)\n",
    "    my_str = re.sub(r'she\\'s', \"she is\", my_str)\n",
    "    my_str = re.sub(r'that\\'s', \"that is\", my_str)\n",
    "    my_str = re.sub(r'what\\'s', \"what is\", my_str)\n",
    "    my_str = re.sub(r'here\\'s', \"here is\", my_str)\n",
    "    my_str = re.sub(r'there\\'s', \"there is\", my_str)\n",
    "    my_str = re.sub(r'who\\'s', \"who is\", my_str)\n",
    "    my_str = re.sub(r'It\\'s', \"It is\", my_str)\n",
    "    my_str = re.sub(r'He\\'s', \"He is\", my_str)\n",
    "    my_str = re.sub(r'She\\'s', \"She is\", my_str)\n",
    "    my_str = re.sub(r'Shat\\'s', \"That is\", my_str)\n",
    "    my_str = re.sub(r'What\\'s', \"What is\", my_str)\n",
    "    my_str = re.sub(r'Here\\'s', \"Here is\", my_str)\n",
    "    my_str = re.sub(r'There\\'s', \"There is\", my_str)\n",
    "    my_str = re.sub(r\"Who's\", \"Who is\", my_str)\n",
    "    return my_str\n",
    "\n",
    "\n",
    "def apostrophe_s_replace(my_str):\n",
    "    \"\"\"Remove 's at the end of words in my_string. Best to run this after short_form_replace()\"\"\"\n",
    "    my_str = re.sub(r\"([A-z])\\'s\", r\"\\1\", my_str)\n",
    "    return my_str\n",
    "\n",
    "\n",
    "def hyphen_replace(my_str):\n",
    "    \"\"\"Replace hyphens in hyphenated words with a space\"\"\"\n",
    "    my_str = re.sub(r\"([A-z])(\\-)([A-z])\", r\"\\1 \\3\", my_str)\n",
    "    my_str = re.sub(r\"([A-z])(\\-)(\\s)([A-z])\", r\"\\1 \\4\", my_str)\n",
    "    return my_str\n",
    "\n",
    "\n",
    "def forward_slash_replace(my_str):\n",
    "    \"\"\"Replace forward slashes in combined words with a space.\"\"\"\n",
    "    my_str = re.sub(r\"([A-z])(/)([A-z])\", r\"\\1 \\3\", my_str)\n",
    "    my_str = my_str.replace(')/', ' ')\n",
    "    my_str = my_str.replace('/(', ' ')\n",
    "    return my_str\n",
    "\n",
    "\n",
    "def replace_apostrophes(my_str):\n",
    "    \"\"\"Tidy up my_string for all apostrophe-related issues.\"\"\"\n",
    "    my_str = replace_non_utf8_apostrophes(my_str)\n",
    "    my_str = short_form_replace(my_str)\n",
    "    my_str = apostrophe_s_replace(my_str)\n",
    "    return my_str\n",
    "\n",
    "\n",
    "def clean_text(my_str):\n",
    "    \"\"\"Clean my_string for all known string issues to prepare for text analysis\"\"\"\n",
    "    my_str = replace_apostrophes(my_str)\n",
    "    my_str = hyphen_replace(my_str)\n",
    "    my_str = forward_slash_replace(my_str)\n",
    "    return my_str\n",
    "\n",
    "\n",
    "def replace_non_utf8_apostrophes(my_str):\n",
    "    \"\"\"Replace most common non-utf-8 apostrophes with utf-8 apostrophes\"\"\"\n",
    "    apostrophes = [str(b'\\xe2\\x80\\x98'), str(b'\\xe2\\x80\\x99')]\n",
    "    for i in range(len(my_str)):\n",
    "        if str(my_str[i].encode('utf-8')) in apostrophes:\n",
    "            my_str = my_str[:i] + \"'\" + my_str[i+1:]\n",
    "    return my_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import stop words list\n",
    "stop_words = get_dictionary(LIBRARY_PATH,'stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seddont/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create English stop words list from NLTK\n",
    "nltk_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = sorted(list(set(stop_words + nltk_stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filename_components(file_path, i):\n",
    "    file_name_and_ext = os.path.basename(file_path)\n",
    "    name_only = os.path.splitext(file_name_and_ext)[0]\n",
    "    date, file_num, ticker = name_only.split(\"_\")\n",
    "    \n",
    "    return (ticker, date, file_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --index-url=http://pypi.python.org/simple/ --trusted-host pypi.python.org gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install --index-url=http://pypi.python.org/simple/ --trusted-host pypi.python.org smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_number(token):\n",
    "    if token.isdigit():\n",
    "        return \"NUM\"\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay', u'current', 'end', 'year', 'NUM']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = ['okay', u'current', 'end', 'year', '2011']\n",
    "[replace_number(t) for t in test_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testLDA_pre_process_document(doc):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # This finds letters only and breaks things up at each non-letters character\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    lower_doc = doc.lower()\n",
    "    clean_lower_doc = remove_square_brackets(lower_doc)\n",
    "    clean_doc = clean_text(clean_lower_doc)\n",
    "    tokens = tokenizer.tokenize(clean_doc)\n",
    "    stopped_tokens = [t for t in tokens if not t in stop_words]\n",
    "    stemmed_tokens = [porter_stemmer.stem(i) for i in stopped_tokens]\n",
    "    replaced_num_tokens = [replace_number(t) for t in stemmed_tokens]\n",
    "    return replaced_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn topic model, apply to Q&A pairs and evaluate similarity of Qs and As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_raw_qa(source_list, output_file):\n",
    "    '''Extract raw Q and A text from a list of files and put a structured dictionary\n",
    "       of the extracted Q & A pairs into an output file.\n",
    "       \n",
    "       Returns a dictionary of raw Q&A pairs'''\n",
    "    \n",
    "    rawtext_qa = defaultdict(dict)\n",
    "    i = 0\n",
    "    report_every = 500\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for f in source_list:\n",
    "    \n",
    "        (ticker, date, file_num) = get_filename_components(f, i)\n",
    "\n",
    "        file_id = ticker+\"_\"+date\n",
    "\n",
    "        ts = read_transcript(f)\n",
    "\n",
    "        # Get Company Participants\n",
    "        start = find_full_string_index(ts, CO_PART_HEADERS)\n",
    "        end = find_full_string_index(ts, OTH_PART_HEADERS) - 1\n",
    "        if start != -1 and end != -2: \n",
    "            co_part_names = get_parts(ts, start, end)\n",
    "\n",
    "            # Get QA Section\n",
    "            start = find_full_string_index(ts, QA_SECTION_HEADERS)\n",
    "            end = find_disclaimer_index(ts, DISCLAIMER)\n",
    "            QA = get_QA(ts, start, end)\n",
    "\n",
    "            # get the QA pairs with a condition to ensure both of at least some length\n",
    "            q_number = 0\n",
    "            for question in QA:\n",
    "\n",
    "                # store indexed rawtext for easier inspection later\n",
    "                rawtext_qa[file_id][q_number] = (question, QA[question])\n",
    "                \n",
    "                q_number += 1\n",
    "\n",
    "        i += 1\n",
    "        if i % report_every == 0:\n",
    "            print \"Processed \", i, \"transcripts in \", time.time() - start_time, \"seconds\"\n",
    "            \n",
    "    # Also store the rawtext pairs, for easier inspection later\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(json.dumps(rawtext_qa))\n",
    "    \n",
    "    print \"Finished extracting q and a text from all\", i, \"transcripts in \", time.time() - start_time, \"seconds\"\n",
    "    \n",
    "    return rawtext_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model_spec(model_spec):\n",
    "    '''Saves a model spec as a JSON in its specified directory'''\n",
    "    if not os.path.exists(LIBRARY_PATH + model_spec[\"model_directory\"]):\n",
    "        os.mkdir(LIBRARY_PATH + model_spec[\"model_directory\"])\n",
    "\n",
    "    # Store the processed tokens in qa_pairs\n",
    "    model_spec_path = LIBRARY_PATH+model_spec[\"model_directory\"]+\"/model_spec.txt\"\n",
    "    with open(model_spec_path, \"w\") as f:\n",
    "        f.write(json.dumps(model_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_raw_qa_pairs(rawtext_qa, model_spec):\n",
    "    '''Turns raw text q and a pairs into processed token sequences using prepro_func\n",
    "       passed to it.\n",
    "       \n",
    "       Returns a dictionary of processed q and a pairs.'''\n",
    "    \n",
    "    qa_pairs = defaultdict(dict)\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    report_every = 2500\n",
    "    \n",
    "    prepro_func = PREPRO_FUNCTIONS[model_spec[\"preprocessing_function\"]]\n",
    "    \n",
    "    # Assemble a unified training texts list from all the qualifying processed Q&A pairs found\n",
    "    for file_id in rawtext_qa:\n",
    "        for q_number in rawtext_qa[file_id]:\n",
    "            \n",
    "            procd_question = prepro_func(rawtext_qa[file_id][q_number][0])\n",
    "            procd_answer = prepro_func(rawtext_qa[file_id][q_number][1])\n",
    "            \n",
    "            qa_pairs[file_id][q_number] = (procd_question, procd_answer)\n",
    "            \n",
    "            i += 1\n",
    "            if i % report_every == 0:\n",
    "                print \"Processed \", i, \"pairs in \", time.time() - start_time, \"seconds\"\n",
    "                    \n",
    "    print \"Finished all\", i, \"raw q and a pairs in \", time.time() - start_time, \"seconds\"\n",
    "            \n",
    "    return qa_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_texts(qa_pairs, model_spec):\n",
    "    '''Assembles a unified set of training texts, that conforms with the \n",
    "       model specification.'''\n",
    "    \n",
    "    # Assemble a unified training texts list from all the qualifying processed Q&A pairs found\n",
    "    texts = []\n",
    "    min_sequence_length = model_spec[\"min_sequence_length\"]\n",
    "    \n",
    "    count_accepted = 0\n",
    "    count_rejected = 0\n",
    "    \n",
    "    for file_id in qa_pairs:\n",
    "        for q_number in qa_pairs[file_id]:\n",
    "            question, answer = qa_pairs[file_id][q_number]\n",
    "            if (len(question) > min_sequence_length and\n",
    "                len(answer) > min_sequence_length):\n",
    "                texts.append(question)\n",
    "                texts.append(answer)\n",
    "                count_accepted += 1\n",
    "            else:\n",
    "                count_rejected +=1\n",
    "                \n",
    "    print count_accepted, \"pairs met minimum length.\"\n",
    "    print count_rejected, \"were rejected.\"\n",
    "                \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_LDA_model(model_spec, texts, passes = 10, iterations = 50):\n",
    "    '''Learns an LDA model according to the model spec and stores the associated files'''\n",
    "    \n",
    "    model_dir = LIBRARY_PATH+model_spec[\"model_directory\"]\n",
    "    \n",
    "    # Set up logging to file for Gensim progress\n",
    "\n",
    "    # Clear any other logging handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "        \n",
    "    logging.basicConfig(filename=model_dir+'/lda_logfile.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n",
    "   \n",
    "    # Convert the texts from all the Questions and Answers into integer form and save\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.save(model_dir+\"/dictionary.txt\")\n",
    "    \n",
    "    # Create and save corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    with open(model_dir+\"/corpus.txt\", \"w\") as f:\n",
    "        f.write(json.dumps(corpus))\n",
    "        \n",
    "    # Fit the LDA model\n",
    "\n",
    "    start_time = time.time()\n",
    "    print \"fitting model\"\n",
    "    # Fit LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                               num_topics=model_spec[\"num_topics\"],\n",
    "                                               id2word = dictionary,\n",
    "                                               passes=passes,\n",
    "                                               iterations = iterations,\n",
    "                                               eval_every = 10)\n",
    "    \n",
    "    print \"model fitting took\", time.time() - start_time, \"seconds\"\n",
    "    \n",
    "    # Remove logging handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    # Save the learned model to file\n",
    "    ldamodel.save(model_dir + \"/full_model\")\n",
    "    \n",
    "    # Save a UTC timestamp into the directory\n",
    "    model_run_at = str(datetime.datetime.utcnow())\n",
    "    with open(model_dir+\"/model_runtime.txt\", \"w\") as f:\n",
    "        f.write(model_run_at)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_hellinger_sims(model_spec):\n",
    "    '''Calc hellinger similarities for a model and its data.'''\n",
    "\n",
    "    hellinger_sims = defaultdict(dict)\n",
    "    \n",
    "    model_dir = LIBRARY_PATH+model_spec[\"model_directory\"]\n",
    "    qa_dir = LIBRARY_PATH+model_spec[\"qa_pair_directory\"]\n",
    "    \n",
    "    with open(qa_dir+\"/qa_pairs.txt\", \"r\") as f:\n",
    "        qa_pairs = json.loads(f.read())\n",
    "        \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel.load(model_dir+\"/full_model\")\n",
    "    dictionary = gensim.corpora.dictionary.Dictionary.load(model_dir+\"/dictionary.txt\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    report_every = 5000\n",
    "    \n",
    "    min_sequence_length = model_spec[\"min_sequence_length\"]\n",
    "\n",
    "    for file_id in qa_pairs:\n",
    "        for q_number in qa_pairs[file_id]:\n",
    "            question, answer = qa_pairs[file_id][q_number]\n",
    "            if (len(question) > min_sequence_length and\n",
    "                len(answer) > min_sequence_length):\n",
    "                q_bow = dictionary.doc2bow(question)\n",
    "                a_bow = dictionary.doc2bow(answer)\n",
    "                lda_q_bow = ldamodel[q_bow]\n",
    "                lda_a_bow = ldamodel[a_bow]\n",
    "                hellinger_sims[file_id][q_number] = hellinger(lda_q_bow, lda_a_bow)\n",
    "            i += 1\n",
    "            if i % report_every == 0:\n",
    "                print \"Processed \", i, \"pairs in\", time.time() - start_time\n",
    "    print \"Finished in\", time.time() - start_time\n",
    "    \n",
    "    # Store the corpus to file for recreating model later if wanted\n",
    "    with open(model_dir+\"/hell_sims.txt\", \"w\") as f:\n",
    "        f.write(json.dumps(hellinger_sims))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring from a saved state\n",
    "\n",
    "Since training the models and processing the text is quite time consuming, it's useful to be able to restore a saved model.  Gensim makes saving and restoring a model easy, but we will often want to also be able to look at the data associated with that model.  This data may not always be the same, because we may apply different preprocessing to the raw transcript text to test the impact of that on the quality of the models.\n",
    "\n",
    "We also may want to generate multiple models from the same processed set of data with different LDA hyperparameters (e.g. number of topics) so the same inpiut data might be associated with multiple LDA models.\n",
    "\n",
    "To make this easy during experimentation this is a class that recovers a saved state from files on disk.  The state is specified by the model_spec dictionary of files so it is very easy to handle any combination of data and models (as long as we kept track in the first place of what data created what model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Saved_state():\n",
    "    '''Represents a saved state that includes an LDA model and the data used to create it.\n",
    "    \n",
    "       Instantiated with a model_spec dictionary that locates the files to \n",
    "       recreate the saved state and includes a description.\n",
    "       \n",
    "       Dictionary needs to contain:\n",
    "       \n",
    "       model_files\n",
    "       qa_pairs_file\n",
    "       raw_qa_text_file\n",
    "       corpus_file\n",
    "       hellinger_file\n",
    "       '''\n",
    "    def __init__(self, model_spec):\n",
    "        \n",
    "        model_dir = LIBRARY_PATH+model_spec[\"model_directory\"]\n",
    "        qa_dir = LIBRARY_PATH+model_spec[\"qa_pair_directory\"]\n",
    "        \n",
    "        self.ldamodel = gensim.models.ldamodel.LdaModel.load(model_dir+\"/full_model\")\n",
    "        \n",
    "        self.dictionary = gensim.corpora.dictionary.Dictionary.load(model_dir+\"/dictionary.txt\")\n",
    "        \n",
    "        with open(qa_dir+\"/qa_pairs.txt\", \"r\") as f:\n",
    "            self.qa_pairs = json.loads(f.read())\n",
    "            \n",
    "        with open(LIBRARY_PATH + \"/raw_qa_data.txt\", \"r\") as f:\n",
    "            self.raw_qa_text = json.loads(f.read())\n",
    "            \n",
    "        with open(model_dir+\"/corpus.txt\", \"r\") as f:\n",
    "            self.corpus = json.loads(f.read())\n",
    "            \n",
    "        with open(model_dir+\"/hell_sims.txt\", \"r\") as f:\n",
    "            self.hellinger_sims = json.loads(f.read())\n",
    "            \n",
    "        with open(model_dir+\"/model_runtime.txt\", \"r\") as f:\n",
    "            self.model_runtime = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting information to inspect topics and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_spec = {\"model_directory\": \"saved_models/topic10_minlength20_base\",\n",
    "              \"qa_pair_directory\": \"saved_models/standard_preproc\",\n",
    "              \"preprocessing_function\": \"testLDA_pre_process_document\",\n",
    "              \"min_sequence_length\": 20,\n",
    "              \"num_topics\": 10,\n",
    "              \"description\": \"Topics 10, Min Length 20, Standard preprocessing\"}\n",
    "\n",
    "# maps function specs to function object\n",
    "PREPRO_FUNCTIONS = {\"testLDA_pre_process_document\": testLDA_pre_process_document}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a model with higher number of iterations to compare convergence performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'Topics 10, Min Length 10, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 10,\n",
       "  'model_directory': 'saved_models/top10_len10_prebase_ps20_it100',\n",
       "  'num_topics': 10,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 10, Min Length 20, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 20,\n",
       "  'model_directory': 'saved_models/top10_len20_prebase_ps20_it100',\n",
       "  'num_topics': 10,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 10, Min Length 40, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 40,\n",
       "  'model_directory': 'saved_models/top10_len40_prebase_ps20_it100',\n",
       "  'num_topics': 10,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 20, Min Length 10, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 10,\n",
       "  'model_directory': 'saved_models/top20_len10_prebase_ps20_it100',\n",
       "  'num_topics': 20,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 20, Min Length 20, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 20,\n",
       "  'model_directory': 'saved_models/top20_len20_prebase_ps20_it100',\n",
       "  'num_topics': 20,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 20, Min Length 40, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 40,\n",
       "  'model_directory': 'saved_models/top20_len40_prebase_ps20_it100',\n",
       "  'num_topics': 20,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 40, Min Length 10, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 10,\n",
       "  'model_directory': 'saved_models/top40_len10_prebase_ps20_it100',\n",
       "  'num_topics': 40,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 40, Min Length 20, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 20,\n",
       "  'model_directory': 'saved_models/top40_len20_prebase_ps20_it100',\n",
       "  'num_topics': 40,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'},\n",
       " {'description': 'Topics 40, Min Length 40, base preprocessing, passes20, iterations100',\n",
       "  'iterations': 100,\n",
       "  'min_sequence_length': 40,\n",
       "  'model_directory': 'saved_models/top40_len40_prebase_ps20_it100',\n",
       "  'num_topics': 40,\n",
       "  'passes': 20,\n",
       "  'preprocessing_function': 'testLDA_pre_process_document',\n",
       "  'qa_pair_directory': 'saved_models/standard_preproc'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of model specs to run\n",
    "\n",
    "num_topics_list = [10, 20, 40]\n",
    "min_length_list = [10, 20, 40]\n",
    "pre_pro_list = [(\"base\", \"testLDA_pre_process_document\")]\n",
    "passes_list = [20]\n",
    "iterations_list = [100]\n",
    "\n",
    "model_spec_list = []\n",
    "\n",
    "for num_topics in num_topics_list:\n",
    "    for min_length in min_length_list:\n",
    "        for pre_pro in pre_pro_list:\n",
    "            for passes in passes_list:\n",
    "                for iterations in iterations_list:\n",
    "                \n",
    "                    dir_name = \"top\"+str(num_topics)+\\\n",
    "                               \"_len\"+str(min_length)+\\\n",
    "                               \"_pre\"+pre_pro[0]+\\\n",
    "                               \"_ps\"+str(passes)+\\\n",
    "                               \"_it\"+str(iterations)\n",
    "\n",
    "                    model_dir = \"saved_models/\"+dir_name\n",
    "\n",
    "                    model_spec = {\"model_directory\": model_dir,\n",
    "                  \"qa_pair_directory\": \"saved_models/standard_preproc\",\n",
    "                  \"preprocessing_function\": pre_pro[1],\n",
    "                  \"min_sequence_length\": min_length,\n",
    "                  \"num_topics\": num_topics,\n",
    "                  \"passes\": passes,\n",
    "                  \"iterations\": iterations,\n",
    "                  \"description\": \"Topics \"+str(num_topics)+\\\n",
    "                                  \", Min Length \"+str(min_length)+\\\n",
    "                                  \", \"+pre_pro[0]+\" preprocessing\"+\\\n",
    "                                  \", passes\"+str(passes)+\\\n",
    "                                  \", iterations\"+str(iterations)}\n",
    "\n",
    "                    model_spec_list.append(model_spec)\n",
    "\n",
    "model_spec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50433 pairs met minimum length.\n",
      "34373 were rejected.\n",
      "fitting model\n",
      "model fitting took 2595.53394294 seconds\n",
      "Processed  5000 pairs in 6.21942782402\n",
      "Processed  10000 pairs in 12.4161520004\n",
      "Processed  15000 pairs in 18.8181989193\n",
      "Processed  20000 pairs in 24.906886816\n",
      "Processed  25000 pairs in 31.0879378319\n",
      "Processed  30000 pairs in 37.1491379738\n",
      "Processed  35000 pairs in 43.3238339424\n",
      "Processed  40000 pairs in 49.5357439518\n",
      "Processed  45000 pairs in 55.7631449699\n",
      "Processed  50000 pairs in 61.9721388817\n",
      "Processed  55000 pairs in 67.950168848\n",
      "Processed  60000 pairs in 74.2484929562\n",
      "Processed  65000 pairs in 80.2668848038\n",
      "Processed  70000 pairs in 86.1547529697\n",
      "Processed  75000 pairs in 92.6742429733\n",
      "Processed  80000 pairs in 98.6693148613\n",
      "Finished in 104.354513884\n",
      "37304 pairs met minimum length.\n",
      "47502 were rejected.\n",
      "fitting model\n",
      "model fitting took 2063.00563502 seconds\n",
      "Processed  5000 pairs in 5.23531413078\n",
      "Processed  10000 pairs in 10.4128570557\n",
      "Processed  15000 pairs in 15.6498150826\n",
      "Processed  20000 pairs in 20.6930520535\n",
      "Processed  25000 pairs in 25.9081370831\n",
      "Processed  30000 pairs in 31.0932319164\n",
      "Processed  35000 pairs in 36.1552429199\n",
      "Processed  40000 pairs in 41.2990789413\n",
      "Processed  45000 pairs in 46.4660229683\n",
      "Processed  50000 pairs in 51.7017090321\n",
      "Processed  55000 pairs in 56.761079073\n",
      "Processed  60000 pairs in 62.0466980934\n",
      "Processed  65000 pairs in 66.9072329998\n",
      "Processed  70000 pairs in 71.6966879368\n",
      "Processed  75000 pairs in 77.3222460747\n",
      "Processed  80000 pairs in 82.2785689831\n",
      "Finished in 86.948018074\n",
      "13245 pairs met minimum length.\n",
      "71561 were rejected.\n",
      "fitting model\n",
      "model fitting took 888.248628139 seconds\n",
      "Processed  5000 pairs in 2.13743019104\n",
      "Processed  10000 pairs in 4.53645014763\n",
      "Processed  15000 pairs in 6.83402609825\n",
      "Processed  20000 pairs in 8.85990905762\n",
      "Processed  25000 pairs in 11.032323122\n",
      "Processed  30000 pairs in 13.1037180424\n",
      "Processed  35000 pairs in 15.163506031\n",
      "Processed  40000 pairs in 17.3321681023\n",
      "Processed  45000 pairs in 19.4432611465\n",
      "Processed  50000 pairs in 21.6928582191\n",
      "Processed  55000 pairs in 23.5947132111\n",
      "Processed  60000 pairs in 25.7707362175\n",
      "Processed  65000 pairs in 27.8028390408\n",
      "Processed  70000 pairs in 29.7161240578\n",
      "Processed  75000 pairs in 32.2994081974\n",
      "Processed  80000 pairs in 34.3524332047\n",
      "Finished in 36.2475922108\n",
      "50433 pairs met minimum length.\n",
      "34373 were rejected.\n",
      "fitting model\n",
      "model fitting took 3145.14367414 seconds\n",
      "Processed  5000 pairs in 7.73156404495\n",
      "Processed  10000 pairs in 15.4376769066\n",
      "Processed  15000 pairs in 22.993694067\n",
      "Processed  20000 pairs in 30.5884940624\n",
      "Processed  25000 pairs in 38.3610579967\n",
      "Processed  30000 pairs in 46.1832480431\n",
      "Processed  35000 pairs in 53.8386650085\n",
      "Processed  40000 pairs in 61.5498459339\n",
      "Processed  45000 pairs in 69.3946199417\n",
      "Processed  50000 pairs in 77.2150609493\n",
      "Processed  55000 pairs in 84.7368929386\n",
      "Processed  60000 pairs in 92.6457369328\n",
      "Processed  65000 pairs in 100.186104059\n",
      "Processed  70000 pairs in 107.680947065\n",
      "Processed  75000 pairs in 115.798966885\n",
      "Processed  80000 pairs in 123.328660011\n",
      "Finished in 130.546846867\n",
      "37304 pairs met minimum length.\n",
      "47502 were rejected.\n",
      "fitting model\n",
      "model fitting took 2407.33565307 seconds\n",
      "Processed  5000 pairs in 6.09651303291\n",
      "Processed  10000 pairs in 12.1211121082\n",
      "Processed  15000 pairs in 18.1426329613\n",
      "Processed  20000 pairs in 24.0167479515\n",
      "Processed  25000 pairs in 30.0887751579\n",
      "Processed  30000 pairs in 36.0718250275\n",
      "Processed  35000 pairs in 42.0556759834\n",
      "Processed  40000 pairs in 48.0545501709\n",
      "Processed  45000 pairs in 54.2362799644\n",
      "Processed  50000 pairs in 60.3620779514\n",
      "Processed  55000 pairs in 66.3018980026\n",
      "Processed  60000 pairs in 72.5469560623\n",
      "Processed  65000 pairs in 78.2814331055\n",
      "Processed  70000 pairs in 83.9750900269\n",
      "Processed  75000 pairs in 90.4792439938\n",
      "Processed  80000 pairs in 96.3762590885\n",
      "Finished in 101.874655008\n",
      "13245 pairs met minimum length.\n",
      "71561 were rejected.\n",
      "fitting model\n",
      "model fitting took 991.696969032 seconds\n",
      "Processed  5000 pairs in 2.37878799438\n",
      "Processed  10000 pairs in 4.81040501595\n",
      "Processed  15000 pairs in 7.27025914192\n",
      "Processed  20000 pairs in 9.53438520432\n",
      "Processed  25000 pairs in 11.9755940437\n",
      "Processed  30000 pairs in 14.3339290619\n",
      "Processed  35000 pairs in 16.6808030605\n",
      "Processed  40000 pairs in 19.1045310497\n",
      "Processed  45000 pairs in 21.4900121689\n",
      "Processed  50000 pairs in 23.9992721081\n",
      "Processed  55000 pairs in 26.1905741692\n",
      "Processed  60000 pairs in 28.6672542095\n",
      "Processed  65000 pairs in 30.9192080498\n",
      "Processed  70000 pairs in 33.0760200024\n",
      "Processed  75000 pairs in 35.9251790047\n",
      "Processed  80000 pairs in 38.2395861149\n",
      "Finished in 40.2915670872\n",
      "50433 pairs met minimum length.\n",
      "34373 were rejected.\n",
      "fitting model\n",
      "model fitting took 4071.55094099 seconds\n",
      "Processed  5000 pairs in 10.2350108624\n",
      "Processed  10000 pairs in 20.4583849907\n",
      "Processed  15000 pairs in 30.4685919285\n",
      "Processed  20000 pairs in 40.6344988346\n",
      "Processed  25000 pairs in 50.9005019665\n",
      "Processed  30000 pairs in 61.0124208927\n",
      "Processed  35000 pairs in 71.0971338749\n",
      "Processed  40000 pairs in 81.4169318676\n",
      "Processed  45000 pairs in 91.7624688148\n",
      "Processed  50000 pairs in 102.114946842\n",
      "Processed  55000 pairs in 111.956408978\n",
      "Processed  60000 pairs in 122.435802937\n",
      "Processed  65000 pairs in 132.335182905\n",
      "Processed  70000 pairs in 142.085832834\n",
      "Processed  75000 pairs in 153.007804871\n",
      "Processed  80000 pairs in 162.932458878\n",
      "Finished in 172.333196878\n",
      "37304 pairs met minimum length.\n",
      "47502 were rejected.\n",
      "fitting model\n",
      "model fitting took 3063.25689816 seconds\n",
      "Processed  5000 pairs in 7.65300893784\n",
      "Processed  10000 pairs in 15.3066818714\n",
      "Processed  15000 pairs in 22.992980957\n",
      "Processed  20000 pairs in 30.4733929634\n",
      "Processed  25000 pairs in 38.4071669579\n",
      "Processed  30000 pairs in 46.2015838623\n",
      "Processed  35000 pairs in 53.8807280064\n",
      "Processed  40000 pairs in 61.7860798836\n",
      "Processed  45000 pairs in 69.6443948746\n",
      "Processed  50000 pairs in 77.6100330353\n",
      "Processed  55000 pairs in 85.2131419182\n",
      "Processed  60000 pairs in 93.3184409142\n",
      "Processed  65000 pairs in 100.807844877\n",
      "Processed  70000 pairs in 108.048925877\n",
      "Processed  75000 pairs in 116.529716969\n",
      "Processed  80000 pairs in 124.11239481\n",
      "Finished in 131.204653978\n",
      "13245 pairs met minimum length.\n",
      "71561 were rejected.\n",
      "fitting model\n",
      "model fitting took 1204.94921303 seconds\n",
      "Processed  5000 pairs in 2.96173286438\n",
      "Processed  10000 pairs in 6.03237199783\n",
      "Processed  15000 pairs in 9.17754602432\n",
      "Processed  20000 pairs in 11.9317550659\n",
      "Processed  25000 pairs in 14.9602229595\n",
      "Processed  30000 pairs in 17.8045899868\n",
      "Processed  35000 pairs in 20.7087578773\n",
      "Processed  40000 pairs in 23.6824200153\n",
      "Processed  45000 pairs in 26.6168458462\n",
      "Processed  50000 pairs in 29.719301939\n",
      "Processed  55000 pairs in 32.399518013\n",
      "Processed  60000 pairs in 35.451969862\n",
      "Processed  65000 pairs in 38.3270380497\n",
      "Processed  70000 pairs in 40.9507069588\n",
      "Processed  75000 pairs in 44.4812150002\n",
      "Processed  80000 pairs in 47.340597868\n",
      "Finished in 49.9357199669\n"
     ]
    }
   ],
   "source": [
    "# Can just load already preprocessed qa pairs as applying standard preprocessing\n",
    "qa_dir = LIBRARY_PATH+model_spec[\"qa_pair_directory\"]\n",
    "with open(qa_dir+\"/qa_pairs.txt\", \"r\") as f:\n",
    "    qa_pairs = json.loads(f.read())\n",
    "\n",
    "for model_spec in model_spec_list:\n",
    "    \n",
    "    save_model_spec(model_spec)\n",
    "\n",
    "    texts = select_texts(qa_pairs, model_spec)\n",
    "\n",
    "    learn_LDA_model(model_spec, texts, passes = model_spec[\"passes\"], iterations = model_spec[\"iterations\"])\n",
    "\n",
    "    calc_hellinger_sims(model_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_logfile(loglines):\n",
    "    '''Parses a gensim logfile to return a list of key information from each pass.\n",
    "       List contains\n",
    "       pass number\n",
    "       per word bound\n",
    "       perplexity\n",
    "       held out size\n",
    "       number converged\n",
    "       number out of which converged\n",
    "       '''\n",
    "\n",
    "    match_doc = re.compile(\"(-*\\d+) documents\")\n",
    "    match_perp = re.compile(\"(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity .* (\\d+) documents\")\n",
    "    match_pass = re.compile(\"pass (\\d+).*document #(\\d+)\")\n",
    "    match_converged = re.compile(\"(\\d+)\\/(\\d+) documents converged\")\n",
    "\n",
    "    perp_list = []\n",
    "\n",
    "    for i in range(len(loglines)):\n",
    "        if \"running online\" in loglines[i]:\n",
    "            # summary of data for the the model run\n",
    "            total_docs = match_doc.findall(loglines[i])[0]\n",
    "        if \"perplexity estimate\" in loglines[i]:\n",
    "            if \"PROGRESS\" in loglines[i+1]:\n",
    "                progress_data = match_pass.findall(loglines[i+1])\n",
    "                doc_num = progress_data[0][1]\n",
    "                # only add data for the end of each complete pass\n",
    "                if doc_num == total_docs:\n",
    "                    pass_num = progress_data[0][0]\n",
    "                    perp_data = match_perp.findall(loglines[i])\n",
    "                    word_bound = perp_data[0][0]\n",
    "                    perplexity = perp_data[0][1]\n",
    "                    held_out = perp_data[0][2]\n",
    "                    converged_data = match_converged.findall(loglines[i+3])\n",
    "                    converged = converged_data[0][0]\n",
    "                    out_of = converged_data[0][1]\n",
    "                    perp_list.append((pass_num, word_bound, perplexity, held_out, converged, out_of))\n",
    "                    \n",
    "    return perp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', '-7.110', '138.2', '866', '848', '866'),\n",
       " ('1', '-7.087', '135.9', '866', '855', '866'),\n",
       " ('2', '-7.081', '135.4', '866', '854', '866'),\n",
       " ('3', '-7.079', '135.2', '866', '853', '866'),\n",
       " ('4', '-7.077', '135.1', '866', '854', '866'),\n",
       " ('5', '-7.076', '135.0', '866', '851', '866'),\n",
       " ('6', '-7.075', '134.8', '866', '855', '866'),\n",
       " ('7', '-7.074', '134.7', '866', '854', '866'),\n",
       " ('8', '-7.073', '134.7', '866', '859', '866'),\n",
       " ('9', '-7.073', '134.6', '866', '855', '866'),\n",
       " ('10', '-7.072', '134.5', '866', '855', '866'),\n",
       " ('11', '-7.072', '134.5', '866', '854', '866'),\n",
       " ('12', '-7.071', '134.4', '866', '856', '866'),\n",
       " ('13', '-7.070', '134.4', '866', '859', '866'),\n",
       " ('14', '-7.070', '134.4', '866', '859', '866'),\n",
       " ('15', '-7.069', '134.3', '866', '852', '866'),\n",
       " ('16', '-7.069', '134.3', '866', '856', '866'),\n",
       " ('17', '-7.068', '134.2', '866', '856', '866'),\n",
       " ('18', '-7.068', '134.1', '866', '858', '866'),\n",
       " ('19', '-7.067', '134.1', '866', '856', '866')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = LIBRARY_PATH+\"saved_models/top20_len10_prebase_ps20_it100\"\n",
    "logfile = model_dir + \"/lda_logfile.log\"\n",
    "with open(logfile) as f:\n",
    "    loglines = f.readlines()\n",
    "\n",
    "parse_logfile(loglines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_models/top10_len10_prebase_ps20_it100\n",
      "('19', '-7.002', '128.1', '866', '862', '866')\n",
      "saved_models/top10_len20_prebase_ps20_it100\n",
      "('19', '-6.986', '126.7', '608', '601', '608')\n",
      "saved_models/top10_len40_prebase_ps20_it100\n",
      "('19', '-6.907', '120.0', '490', '481', '490')\n",
      "saved_models/top20_len10_prebase_ps20_it100\n",
      "('19', '-7.067', '134.1', '866', '856', '866')\n",
      "saved_models/top20_len20_prebase_ps20_it100\n",
      "('19', '-7.021', '129.9', '608', '604', '608')\n",
      "saved_models/top20_len40_prebase_ps20_it100\n",
      "('19', '-6.912', '120.4', '490', '487', '490')\n",
      "saved_models/top40_len10_prebase_ps20_it100\n",
      "('19', '-7.166', '143.6', '866', '862', '866')\n",
      "saved_models/top40_len20_prebase_ps20_it100\n",
      "('19', '-7.123', '139.4', '608', '606', '608')\n",
      "saved_models/top40_len40_prebase_ps20_it100\n",
      "('19', '-6.967', '125.1', '490', '488', '490')\n"
     ]
    }
   ],
   "source": [
    "final_perp = dict()\n",
    "\n",
    "for model_spec in model_spec_list:\n",
    "    model_dir = model_spec[\"model_directory\"]\n",
    "    logfile = LIBRARY_PATH+model_dir+\"/lda_logfile.log\"\n",
    "    with open(logfile) as f:\n",
    "        loglines = f.readlines()\n",
    "    final_perp[model_dir] = parse_logfile(loglines)\n",
    "    print model_dir\n",
    "    print final_perp[model_dir][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
